# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vEMuWQGdcVyoOqviVLLOO9GeHFDeyflo
"""



# Install required packages
!pip install gradio transformers torch accelerate gtts IPython
!pip install --upgrade huggingface_hub

import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from gtts import gTTS
import tempfile
import os
from IPython.display import Audio
import io
import base64

class GraniteTextGenerator:
    def __init__(self):
        self.model_name = "ibm-granite/granite-3.2-2b-instruct"
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")

    def load_model(self):
        """Load the Granite 3.2.2b model"""
        try:
            print("Loading Granite 3.2.2b model...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )

            # Set padding token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print("Model loaded successfully!")
            return True
        except Exception as e:
            print(f"Error loading model: {e}")
            return False

    def generate_text(self, prompt, max_length=200, temperature=0.7, top_p=0.9):
        """Generate text using the Granite model"""
        if not self.model or not self.tokenizer:
            return "Error: Model not loaded"

        try:
            # Format prompt for instruction following
            formatted_prompt = f"<|user|>\n{prompt}\n<|assistant|>\n"

            # Tokenize input
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt", padding=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Generate text
            output_sequences = self.model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                num_return_sequences=1,
                pad_token_id=self.tokenizer.pad_token_id # Use pad_token_id for padding
            )

            # Decode generated text, skipping the prompt tokens
            generated_text = self.tokenizer.decode(output_sequences[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
            return generated_text.strip()

        except Exception as e:
            print(f"Error generating text: {e}")
            return f"Error generating text: {e}"

class TextToSpeechConverter:
    def __init__(self):
        pass

    def convert_text_to_speech(self, text):
        """Convert text to speech using gTTS and return as base64 encoded audio"""
        if not text:
            return None

        try:
            tts = gTTS(text=text, lang='en')
            audio_stream = io.BytesIO()
            tts.write_to_fp(audio_stream)
            audio_stream.seek(0)
            audio_bytes = audio_stream.read()
            audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
            return f"data:audio/mpeg;base64,{audio_base64}"

        except Exception as e:
            print(f"Error converting text to speech: {e}")
            return None

# Initialize the models
text_generator = GraniteTextGenerator()
tts_converter = TextToSpeechConverter()

# Load the Granite model
model_loaded = text_generator.load_model()

# Gradio Interface
def generate_and_speak(prompt):
    if not model_loaded:
        return "Model failed to load.", None

    generated_text = text_generator.generate_text(prompt)
    audio_base64 = tts_converter.convert_text_to_speech(generated_text)

    if audio_base64:
        # Gradio expects the audio to be in a specific format (e.g., tuple of (sample_rate, audio_data))
        # For base64, we need to handle it differently. Gradio's Audio component can take a base64 string directly.
        return generated_text, audio_base64
    else:
        return generated_text, None

iface = gr.Interface(
    fn=generate_and_speak,
    inputs=gr.Textbox(lines=2, label="Enter your prompt"),
    outputs=[
        gr.Textbox(label="Generated Text"),
        gr.Audio(label="Generated Audio", type="filepath") # Use type="filepath" with base64 handled internally by Gradio
    ],
    title="Granite Text Generation with TTS",
    description="Generate text using the IBM Granite model and convert it to speech."
)

if __name__ == "__main__":
    iface.launch()